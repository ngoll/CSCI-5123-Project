{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "os.chdir(\"/Users/nikhithagollamudi/Desktop/School/5123/final-project/CSCI-5123-Project/\")\n",
    "\n",
    "#load data\n",
    "user_triplets_df = pd.read_csv(\"resources/data/dataset/user_activity_triplets.csv\", sep=\";\")\n",
    "original_orders_df = pd.read_csv(\"resources/data/dataset/original_orders.csv\", sep=\";\")\n",
    "outfits_df = pd.read_csv(\"resources/data/dataset/outfits.csv\", sep=\";\")\n",
    "\n",
    "#preprocess outfit tags\n",
    "outfits_df[\"outfit_tags\"] = outfits_df[\"outfit_tags\"].apply(eval)\n",
    "outfits_df[\"tag_categories\"] = outfits_df[\"tag_categories\"].apply(eval)\n",
    "\n",
    "#combine triplet data\n",
    "user_triplets_df = pd.concat([user_triplets_df, original_orders_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4949\n",
      "No unique outfit found with groups ['group.4bd4ee24eac8948e82783b15d9404f6b'\n",
      " 'group.4bd4ee24eac8948e82783b15d9404f6b']\n",
      "No unique outfit found with groups ['group.423a23f6717e6d85adac54c051ee9832'\n",
      " 'group.423a23f6717e6d85adac54c051ee9832']\n",
      "No unique outfit found with groups ['group.e0cb0f6e113edc4df8a1e304376734f6'\n",
      " 'group.e0cb0f6e113edc4df8a1e304376734f6']\n",
      "No unique outfit found with groups ['group.384b8170c6a6ddfd568ff7fab5fb49c4'\n",
      " 'group.384b8170c6a6ddfd568ff7fab5fb49c4']\n",
      "No unique outfit found with groups ['group.edb60c2f440a9ac7d0883fb9371c8607'\n",
      " 'group.edb60c2f440a9ac7d0883fb9371c8607']\n",
      "No unique outfit found with groups ['group.a3ab26b5d2f7ef2cf102422a3dde3b46'\n",
      " 'group.a3ab26b5d2f7ef2cf102422a3dde3b46']\n",
      "No unique outfit found with groups ['group.2c7095c075561fe6278f3a2d7c1d6ac9'\n",
      " 'group.2c7095c075561fe6278f3a2d7c1d6ac9']\n",
      "No unique outfit found with groups ['group.ae8da3f0ad6f8ff3f83b2af96e975991'\n",
      " 'group.ae8da3f0ad6f8ff3f83b2af96e975991']\n",
      "No unique outfit found with groups ['group.1bfd2412df50ac58b23bd8f52c6b4b35'\n",
      " 'group.1bfd2412df50ac58b23bd8f52c6b4b35']\n",
      "No unique outfit found with groups ['group.148f5272ecc1480d49191b3923aab5a2'\n",
      " 'group.148f5272ecc1480d49191b3923aab5a2']\n",
      "No unique outfit found with groups ['group.8e50238120d13b31284f151941c2ee81'\n",
      " 'group.8e50238120d13b31284f151941c2ee81']\n",
      "No unique outfit found with groups ['group.a494d07781a1aab0e3a42989288feff2'\n",
      " 'group.a494d07781a1aab0e3a42989288feff2']\n",
      "No unique outfit found with groups ['group.a1d284ef1c7035dd14e57eba3838a303'\n",
      " 'group.a1d284ef1c7035dd14e57eba3838a303']\n",
      "No unique outfit found with groups ['group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97']\n"
     ]
    }
   ],
   "source": [
    "#train/test splits\n",
    "from src.prepare_train_test_splits import (translate_user_triplets_to_orders, remove_consecutive_duplicates, convert_user_orders_to_train_test_splits)\n",
    "\n",
    "user_triplets_df = remove_consecutive_duplicates(user_triplets_df)\n",
    "user_orders_df = translate_user_triplets_to_orders(user_triplets_df, outfits_df)\n",
    "user_splits_df, user_splits_unique_df = convert_user_orders_to_train_test_splits(user_orders_df, percentage_test=0.3)\n",
    "\n",
    "#filter unique outfits\n",
    "def flatten_lists(series):\n",
    "    unique_items = set()\n",
    "\n",
    "    for item_list in series:\n",
    "        for item in item_list:\n",
    "            unique_items.add(item)\n",
    "    \n",
    "    return unique_items\n",
    "\n",
    "all_train_ids = flatten_lists(user_splits_df[\"train_outfit_ids\"])\n",
    "all_test_ids = flatten_lists(user_splits_df[\"test_outfit_id\"])\n",
    "all_outfit_ids = all_train_ids.union(all_test_ids)\n",
    "outfits_df = outfits_df[outfits_df[\"id\"].isin(all_outfit_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhithagollamudi/Library/Python/3.9/lib/python/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.0002429485321044922 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0027179718017578125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 15,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e6c6c652794276948d97af702038c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tag-based recommendations (Content-Based)\n",
    "mlb = MultiLabelBinarizer()\n",
    "tag_matrix = pd.DataFrame(\n",
    "    mlb.fit_transform(outfits_df[\"outfit_tags\"]),\n",
    "    index=outfits_df[\"id\"],\n",
    "    columns=mlb.classes_\n",
    ")\n",
    "        \n",
    "#store user profiles based on average tag vectors of their training outfits\n",
    "user_profiles = {}\n",
    "for user_index, row in user_splits_df.iterrows():\n",
    "    #filter outfits that exist in the tag matrix\n",
    "    valid_outfit_ids = []\n",
    "    for outfit_id in row[\"train_outfit_ids\"]:\n",
    "        if outfit_id in tag_matrix.index:\n",
    "            valid_outfit_ids.append(outfit_id)\n",
    "    \n",
    "    #calculate the mean tag vector \n",
    "    if valid_outfit_ids:\n",
    "        average_tag_vector = tag_matrix.loc[valid_outfit_ids].mean(axis=0)\n",
    "        user_profiles[user_index] = average_tag_vector\n",
    "\n",
    "\n",
    "user_tag_matrix = pd.DataFrame.from_dict(user_profiles, orient=\"index\").fillna(0)\n",
    "similarity_matrix = cosine_similarity(user_tag_matrix.values, tag_matrix.values)\n",
    "\n",
    "cb_scores = []\n",
    "for user_index, user_id in enumerate(user_tag_matrix.index):\n",
    "    #get similarity scores\n",
    "    user_scores = similarity_matrix[user_index]\n",
    "\n",
    "    #get indices of the 300 most similar outfits\n",
    "    top_outfit_indices = user_scores.argsort()[::-1][:300]\n",
    "\n",
    "    for outfit_index in top_outfit_indices:\n",
    "        outfit_id = tag_matrix.index[outfit_index]\n",
    "        similarity_score = user_scores[outfit_index]\n",
    "        cb_scores.append((user_id, outfit_id, similarity_score))\n",
    "\n",
    "cb_df = pd.DataFrame(cb_scores, columns=[\"user_id\", \"item_id\", \"score\"])\n",
    "cb_df.to_csv(\"tag_embedding_scores.csv\", index=False)\n",
    "\n",
    "#ALS-based recommendations (Collaborative Filtering)\n",
    "train_df = user_splits_df.explode(\"train_outfit_ids\").dropna()\n",
    "train_df[\"user_id\"] = train_df.index\n",
    "train_df[\"value\"] = 1\n",
    "train_df.rename(columns={\"train_outfit_ids\": \"item_id\"}, inplace=True)\n",
    "\n",
    "#encode user and item IDs to ints\n",
    "user_enc = LabelEncoder()\n",
    "item_enc = LabelEncoder()\n",
    "train_df[\"user_idx\"] = user_enc.fit_transform(train_df[\"user_id\"])\n",
    "train_df[\"item_idx\"] = item_enc.fit_transform(train_df[\"item_id\"])\n",
    "\n",
    "user_index_to_user_id = {}\n",
    "item_index_to_item_id = {}\n",
    "\n",
    "#user index mapping\n",
    "for i in range(len(train_df)):\n",
    "    row = train_df.iloc[i]\n",
    "    user_index = row[\"user_idx\"]\n",
    "    user_id = row[\"user_id\"]\n",
    "    user_index_to_user_id[user_index] = user_id\n",
    "\n",
    "#item index mapping\n",
    "for i in range(len(train_df)):\n",
    "    row = train_df.iloc[i]\n",
    "    item_index = row[\"item_idx\"]\n",
    "    item_id = row[\"item_id\"]\n",
    "    item_index_to_item_id[item_index] = item_id\n",
    "\n",
    "#create a sparse matrix and convert it to CSR \n",
    "coo = coo_matrix((train_df[\"value\"], (train_df[\"user_idx\"], train_df[\"item_idx\"])))\n",
    "csr = coo.tocsr()\n",
    "\n",
    "#parameters matched to the ones used by the paper \n",
    "als = AlternatingLeastSquares(factors=128, regularization=0.01, iterations=15)\n",
    "als.fit(csr.T)\n",
    "\n",
    "als_scores = []\n",
    "for user_idx in range(csr.shape[0]):\n",
    "    #get top 300 recommended item indices and their scores \n",
    "    item_indices, scores = als.recommend(user_idx, csr, N=300, filter_already_liked_items=False)\n",
    "    user_id = user_index_to_id[user_idx]\n",
    "\n",
    "    for i in range(len(item_indices)):\n",
    "        item_idx = item_indices[i]\n",
    "        score = scores[i]\n",
    "        item_id = item_index_to_id[item_idx]\n",
    "        als_scores.append((user_id, item_id, float(score)))\n",
    "\n",
    "als_df = pd.DataFrame(als_scores, columns=[\"user_id\", \"item_id\", \"score\"])\n",
    "als_df.to_csv(\"als_recommendation_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hybrid recommendations\n",
    "als_df = pd.read_csv(\"als_recommendation_scores.csv\")\n",
    "cb_df = pd.read_csv(\"tag_embedding_scores.csv\")\n",
    "\n",
    "#normalize scores\n",
    "scaler_als = MinMaxScaler()\n",
    "als_df[\"score_als\"] = scaler_als.fit_transform(als_df[[\"score\"]])\n",
    "\n",
    "scaler_cb = MinMaxScaler()\n",
    "cb_df[\"score_cb\"] = scaler_cb.fit_transform(cb_df[[\"score\"]])\n",
    "\n",
    "merged = pd.merge(als_df[[\"user_id\", \"item_id\", \"score_als\"]], cb_df[[\"user_id\", \"item_id\", \"score_cb\"]], on=[\"user_id\", \"item_id\"])\n",
    "\n",
    "alpha = 0.5\n",
    "#calculate the hybrid score as a weighted average of ALS and CB scores\n",
    "merged[\"score_hybrid\"] = alpha * merged[\"score_als\"] + (1 - alpha) * merged[\"score_cb\"]\n",
    "merged[\"rank\"] = merged.groupby(\"user_id\")[\"score_hybrid\"].rank(ascending=False, method=\"first\")\n",
    "#save the top 100 recommendations\n",
    "top_100 = merged[merged[\"rank\"] <= 100]\n",
    "top_100.to_csv(\"hybrid_top100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Hit Rate @10: 0.0203\n",
      "ID Hit Rate @100: 0.0210\n",
      "Group Hit Rate @10: 0.0280\n",
      "Group Hit Rate @100: 0.0293\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "user_splits_df = pd.read_pickle(\"user_splits_df.pkl\")\n",
    "user_splits_df[\"user_id\"] = user_splits_df.index\n",
    "\n",
    "#reshape the hybrid recommendations so each user_id maps to a list of their top recommended item_ids\n",
    "hybrid_df = pd.read_csv(\"hybrid_top100.csv\")\n",
    "hybrid_grouped = hybrid_df.groupby(\"user_id\")[\"item_id\"].apply(list).reset_index()\n",
    "hybrid_grouped.columns = [\"user_id\", \"hybrid_recommendations\"]\n",
    "\n",
    "#merge and evaluate\n",
    "eval_df = pd.merge(user_splits_df, hybrid_grouped, on=\"user_id\")\n",
    "\n",
    "#format outfit IDs to match the prefixes \n",
    "def format_outfit_ids(outfit_ids):\n",
    "    formatted = []\n",
    "    for outfit_id in outfit_ids:\n",
    "        outfit_str = str(outfit_id)\n",
    "        if not outfit_str.startswith(\"outfit.\"):\n",
    "            outfit_str = \"outfit.\" + outfit_str\n",
    "        formatted.append(outfit_str)\n",
    "    return formatted\n",
    "\n",
    "eval_df[\"hybrid_recommendations\"] = eval_df[\"hybrid_recommendations\"].apply(format_outfit_ids)\n",
    "\n",
    "\n",
    "def evaluate_hit_rate_at_n(test_ids, predicted_ids, n=100):\n",
    "    top_n_predictions = predicted_ids[:n]\n",
    "\n",
    "    #make sure test_ids is a list\n",
    "    is_list_or_array = isinstance(test_ids, (list, np.ndarray))\n",
    "    if not is_list_or_array:\n",
    "        test_ids = [test_ids]\n",
    "\n",
    "    #check for hits \n",
    "    hit_found = False\n",
    "    for test_id in test_ids:\n",
    "        if test_id in top_n_predictions:\n",
    "            hit_found = True\n",
    "            break\n",
    "\n",
    "    return int(hit_found)\n",
    "\n",
    "\n",
    "#ID Hit Rates \n",
    "id_hit_rate_at_10 = []\n",
    "id_hit_rate_at_100 = []\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    hit_10 = evaluate_hit_rate_at_n(row[\"test_outfit_id\"], row[\"hybrid_recommendations\"], n=10)\n",
    "    hit_100 = evaluate_hit_rate_at_n(row[\"test_outfit_id\"], row[\"hybrid_recommendations\"], n=100)\n",
    "    \n",
    "    id_hit_rate_at_10.append(hit_10)\n",
    "    id_hit_rate_at_100.append(hit_100)\n",
    "\n",
    "eval_df[\"id_hit_rate_at_10\"] = id_hit_rate_at_10\n",
    "eval_df[\"id_hit_rate_at_100\"] = id_hit_rate_at_100\n",
    "\n",
    "\n",
    "#map outfit IDs to group IDs \n",
    "predicted_groups_list = []\n",
    "outfit_to_group = outfits_df.set_index(\"id\")[\"group\"].to_dict()\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    group_ids = []\n",
    "    for outfit_id in row[\"hybrid_recommendations\"]:\n",
    "        group_ids.append(outfit_to_group.get(outfit_id, \"\"))\n",
    "    predicted_groups_list.append(group_ids)\n",
    "\n",
    "eval_df[\"predicted_groups\"] = predicted_groups_list\n",
    "\n",
    "\n",
    "#Group Hit Rates \n",
    "group_hit_rate_at_10 = []\n",
    "group_hit_rate_at_100 = []\n",
    "\n",
    "for _, row in eval_df.iterrows():\n",
    "    hit_10 = evaluate_hit_rate_at_n(row[\"test_group\"], row[\"predicted_groups\"], n=10)\n",
    "    hit_100 = evaluate_hit_rate_at_n(row[\"test_group\"], row[\"predicted_groups\"], n=100)\n",
    "    \n",
    "    group_hit_rate_at_10.append(hit_10)\n",
    "    group_hit_rate_at_100.append(hit_100)\n",
    "\n",
    "eval_df[\"group_hit_rate_at_10\"] = group_hit_rate_at_10\n",
    "eval_df[\"group_hit_rate_at_100\"] = group_hit_rate_at_100\n",
    "\n",
    "#results\n",
    "print(f\"ID Hit Rate @10: {eval_df['id_hit_rate_at_10'].mean():.4f}\")\n",
    "print(f\"ID Hit Rate @100: {eval_df['id_hit_rate_at_100'].mean():.4f}\")\n",
    "print(f\"Group Hit Rate @10: {eval_df['group_hit_rate_at_10'].mean():.4f}\")\n",
    "print(f\"Group Hit Rate @100: {eval_df['group_hit_rate_at_100'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra-List Similarity (ILS): 0.4998\n"
     ]
    }
   ],
   "source": [
    "#Intra-List Similarity \n",
    "def compute_ils(tag_matrix, recommendations):\n",
    "    ils_scores = []\n",
    "    for recommended_item_ids in recommendations:\n",
    "        #filter out item IDs not in tag matrix index\n",
    "        valid_item_ids = []\n",
    "        for item_id in recommended_item_ids:\n",
    "            if item_id in tag_matrix.index:\n",
    "                valid_item_ids.append(item_id)\n",
    "\n",
    "        if len(valid_item_ids) < 2:\n",
    "            continue\n",
    "        \n",
    "        tag_vectors = tag_matrix.loc[valid_item_ids].values\n",
    "\n",
    "        #compute cosine similarity matrix \n",
    "        similarity_matrix = cosine_similarity(tag_vectors)\n",
    "\n",
    "        #get the upper triangle of the similarity matrix\n",
    "        upper_triangle_indices = np.triu_indices_from(similarity_matrix, k=1)\n",
    "\n",
    "        if len(upper_triangle_indices[0]) == 0:\n",
    "            continue\n",
    "\n",
    "        #compute the mean pairwise similarity for the item set\n",
    "        upper_triangle_values = similarity_matrix[upper_triangle_indices]\n",
    "        mean_similarity = upper_triangle_values.mean()\n",
    "\n",
    "        ils_scores.append(mean_similarity)\n",
    "\n",
    "    if ils_score:\n",
    "        return np.mean(ils_scores) \n",
    "    else:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "# Rebuild tag_matrix with outfit.id as index \n",
    "tag_matrix_filtered = tag_matrix.loc[tag_matrix.index.intersection(outfits_df[\"id\"])]\n",
    "\n",
    "ils_score = compute_ils(tag_matrix_filtered, eval_df[\"hybrid_recommendations\"])\n",
    "print(f\"Intra-List Similarity (ILS): {ils_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID NDCG @10: 0.0043\n",
      "ID NDCG @100: 0.0039\n",
      "Group NDCG @10: 0.0059\n",
      "Group NDCG @100: 0.0053\n"
     ]
    }
   ],
   "source": [
    "#NDCG\n",
    "def evaluate_ndcg_at_n(test_ids, predicted_ids, n=100):\n",
    "    top_n_predictions = predicted_ids[:n]\n",
    "\n",
    "    #calculate DCG \n",
    "    dcg = 0.0\n",
    "    for rank, predicted_id in enumerate(top_n_predictions):\n",
    "        position = rank + 2  \n",
    "        if predicted_id in test_ids:\n",
    "            gain = 1 / np.log2(position)\n",
    "            dcg += gain\n",
    "\n",
    "    #calculate IDCG \n",
    "    ideal_ranking_length = min(len(test_ids), n)\n",
    "    idcg = 0.0\n",
    "    for rank in range(ideal_ranking_length):\n",
    "        position = rank + 2\n",
    "        idcg += 1 / np.log2(position)\n",
    "\n",
    "    #normalize DCG \n",
    "    if idcg > 0:\n",
    "        ndcg = dcg / idcg\n",
    "    else:\n",
    "        ndcg = 0.0\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "#ID NDCG\n",
    "id_ndcg_at_10 = []\n",
    "id_ndcg_at_100 = []\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    ndcg_10 = evaluate_ndcg_at_n(row[\"test_outfit_id\"], row[\"hybrid_recommendations\"], n=10)\n",
    "    ndcg_100 = evaluate_ndcg_at_n(row[\"test_outfit_id\"], row[\"hybrid_recommendations\"], n=100)\n",
    "    id_ndcg_at_10.append(ndcg_10)\n",
    "    id_ndcg_at_100.append(ndcg_100)\n",
    "\n",
    "eval_df[\"id_ndcg_at_10\"] = id_ndcg_at_10\n",
    "eval_df[\"id_ndcg_at_100\"] = id_ndcg_at_100\n",
    "\n",
    "#Group NDCG\n",
    "group_ndcg_at_10 = []\n",
    "group_ndcg_at_100 = []\n",
    "\n",
    "for idx, row in eval_df.iterrows():\n",
    "    ndcg_10 = evaluate_ndcg_at_n(row[\"test_group\"], row[\"predicted_groups\"], n=10)\n",
    "    ndcg_100 = evaluate_ndcg_at_n(row[\"test_group\"], row[\"predicted_groups\"], n=100)\n",
    "    group_ndcg_at_10.append(ndcg_10)\n",
    "    group_ndcg_at_100.append(ndcg_100)\n",
    "\n",
    "eval_df[\"group_ndcg_at_10\"] = group_ndcg_at_10\n",
    "eval_df[\"group_ndcg_at_100\"] = group_ndcg_at_100\n",
    "\n",
    "#results\n",
    "print(f\"ID NDCG @10: {eval_df['id_ndcg_at_10'].mean():.4f}\")\n",
    "print(f\"ID NDCG @100: {eval_df['id_ndcg_at_100'].mean():.4f}\")\n",
    "print(f\"Group NDCG @10: {eval_df['group_ndcg_at_10'].mean():.4f}\")\n",
    "print(f\"Group NDCG @100: {eval_df['group_ndcg_at_100'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
