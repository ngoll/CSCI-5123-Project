{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "\n",
    "from resources.constants import *\n",
    "\n",
    "pictures_df = pd.read_csv(PICTURE_TRIPLETS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "outfits_df = pd.read_csv(OUTFITS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "user_triplets_df = pd.read_csv(USER_ACTIVITY_TRIPLETS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "\n",
    "# CSV files are read as strings, so we need to convert them to lists\n",
    "outfits_df[\"tag_categories\"] = outfits_df[\"tag_categories\"].apply(eval)\n",
    "outfits_df[\"outfit_tags\"] = outfits_df[\"outfit_tags\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add original orders to user triplets\n",
    "original_orders_df = pd.read_csv(ORIGINAL_ORDERS_CSV_PATH, sep=CSV_SEPARATOR)\n",
    "user_triplets_df = pd.concat([user_triplets_df, original_orders_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4949\n"
     ]
    }
   ],
   "source": [
    "from src.prepare_train_test_splits import translate_user_triplets_to_orders, remove_consecutive_duplicates\n",
    "\n",
    "# Convert triplets into entries for each individual user\n",
    "user_triplets_df = remove_consecutive_duplicates(user_triplets_df)\n",
    "user_orders_df = translate_user_triplets_to_orders(user_triplets_df, outfits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unique outfit found with groups ['group.a494d07781a1aab0e3a42989288feff2'\n",
      " 'group.a494d07781a1aab0e3a42989288feff2']\n",
      "No unique outfit found with groups ['group.423a23f6717e6d85adac54c051ee9832'\n",
      " 'group.423a23f6717e6d85adac54c051ee9832']\n",
      "No unique outfit found with groups ['group.ae8da3f0ad6f8ff3f83b2af96e975991'\n",
      " 'group.ae8da3f0ad6f8ff3f83b2af96e975991']\n",
      "No unique outfit found with groups ['group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97'\n",
      " 'group.9b5204b87abc93f8f0467b0a6a9c6a97']\n",
      "No unique outfit found with groups ['group.148f5272ecc1480d49191b3923aab5a2'\n",
      " 'group.148f5272ecc1480d49191b3923aab5a2']\n",
      "No unique outfit found with groups ['group.1bfd2412df50ac58b23bd8f52c6b4b35'\n",
      " 'group.1bfd2412df50ac58b23bd8f52c6b4b35']\n",
      "No unique outfit found with groups ['group.2c7095c075561fe6278f3a2d7c1d6ac9'\n",
      " 'group.2c7095c075561fe6278f3a2d7c1d6ac9']\n",
      "No unique outfit found with groups ['group.e0cb0f6e113edc4df8a1e304376734f6'\n",
      " 'group.e0cb0f6e113edc4df8a1e304376734f6']\n",
      "No unique outfit found with groups ['group.a1d284ef1c7035dd14e57eba3838a303'\n",
      " 'group.a1d284ef1c7035dd14e57eba3838a303']\n",
      "No unique outfit found with groups ['group.384b8170c6a6ddfd568ff7fab5fb49c4'\n",
      " 'group.384b8170c6a6ddfd568ff7fab5fb49c4']\n",
      "No unique outfit found with groups ['group.8e50238120d13b31284f151941c2ee81'\n",
      " 'group.8e50238120d13b31284f151941c2ee81']\n",
      "No unique outfit found with groups ['group.edb60c2f440a9ac7d0883fb9371c8607'\n",
      " 'group.edb60c2f440a9ac7d0883fb9371c8607']\n",
      "No unique outfit found with groups ['group.4bd4ee24eac8948e82783b15d9404f6b'\n",
      " 'group.4bd4ee24eac8948e82783b15d9404f6b']\n",
      "No unique outfit found with groups ['group.a3ab26b5d2f7ef2cf102422a3dde3b46'\n",
      " 'group.a3ab26b5d2f7ef2cf102422a3dde3b46']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.prepare_train_test_splits import convert_user_orders_to_train_test_splits\n",
    "\n",
    "# Split the data into train and test sets, with one dataframe with no restirictions on outfits in the test data and one that prohibits repeated outfits\n",
    "# It prints any cases in which it is unable to construct a test set with unique outfits.\n",
    "user_splits_df, user_splits_unique_df = convert_user_orders_to_train_test_splits(user_orders_df, percentage_test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_if_train_is_in_test(train_outfit_ids, test_outfit_ids):\n",
    "    contaminated = np.isin(train_outfit_ids, test_outfit_ids).any()\n",
    "    if contaminated:\n",
    "        print(\", \".join(train_outfit_ids) + \"||\" + \", \".join(test_outfit_ids))\n",
    "    return contaminated\n",
    "\n",
    "# Ensure the separation between unique outfits is valid. This should return 0.\n",
    "user_splits_unique_df.apply(lambda x: check_if_train_is_in_test(x[\"train_outfit_ids\"], x[\"test_outfit_id\"]), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.rs_methods\n",
    "# The maximum number of items to recommend\n",
    "NUM_ITEMS = 100\n",
    "\n",
    "# The below code represents the four baseline methods discussed.\n",
    "\n",
    "# Most popular outfits prediction\n",
    "def predict_most_popular(user_splits_df, user_splits_unique_df):\n",
    "    most_popular_train_outfit_ids, most_popular_train_groups = src.rs_methods.get_most_popular_outfits(user_splits_df, NUM_ITEMS)\n",
    "    user_splits_df[\"id_prediction\"] = [most_popular_train_outfit_ids] * len(user_splits_df)\n",
    "    user_splits_df[\"group_prediction\"] = [most_popular_train_groups] * len(user_splits_df)\n",
    "    most_popular_train_outfit_ids, most_popular_train_groups = src.rs_methods.get_most_popular_outfits(user_splits_unique_df, NUM_ITEMS)\n",
    "    user_splits_unique_df[\"id_prediction\"] = [most_popular_train_outfit_ids] * len(user_splits_unique_df)\n",
    "    user_splits_unique_df[\"group_prediction\"] = [most_popular_train_groups] * len(user_splits_unique_df)\n",
    "    return user_splits_df, user_splits_unique_df\n",
    "\n",
    "# Previous rental prediction\n",
    "def predict_previous_rental(user_splits_df, user_splits_unique_df):\n",
    "    user_splits_df[\"id_prediction\"] = user_splits_df[\"train_outfit_ids\"].apply(lambda x: x[-NUM_ITEMS:])\n",
    "    user_splits_df[\"group_prediction\"] = user_splits_df[\"train_group\"].apply(lambda x: x[-NUM_ITEMS:])\n",
    "    user_splits_unique_df[\"id_prediction\"] = user_splits_unique_df[\"train_outfit_ids\"].apply(lambda x: x if len(x) <= NUM_ITEMS else x[-NUM_ITEMS:])\n",
    "    user_splits_unique_df[\"group_prediction\"] = user_splits_unique_df[\"train_group\"].apply(lambda x: x if len(x) <= NUM_ITEMS else x[-NUM_ITEMS:])\n",
    "    return user_splits_df, user_splits_unique_df\n",
    "\n",
    "# Previous rental + most popular outfits prediction\n",
    "def predict_rental_and_most_popular(user_splits_df, user_splits_unique_df):\n",
    "    def pad_with_most_popular(x, pop_outfits):\n",
    "        if len(x) < NUM_ITEMS:\n",
    "            return np.append(x, pop_outfits[:NUM_ITEMS - len(x)])\n",
    "        else:\n",
    "            return x[-NUM_ITEMS:]\n",
    "\n",
    "    most_popular_train_outfit_ids, most_popular_train_groups = src.rs_methods.get_most_popular_outfits(user_splits_df, NUM_ITEMS)\n",
    "    user_splits_df[\"id_prediction\"] = user_splits_df.apply(lambda x: pad_with_most_popular(x[\"train_outfit_ids\"], most_popular_train_outfit_ids), axis=1)\n",
    "    user_splits_df[\"group_prediction\"] = user_splits_df.apply(lambda x: pad_with_most_popular(x[\"train_group\"], most_popular_train_groups), axis=1)\n",
    "    user_splits_unique_df[\"id_prediction\"] = user_splits_unique_df.apply(lambda x: pad_with_most_popular(x[\"train_outfit_ids\"], most_popular_train_outfit_ids), axis=1)\n",
    "    user_splits_unique_df[\"group_prediction\"] = user_splits_unique_df.apply(lambda x: pad_with_most_popular(x[\"train_group\"], most_popular_train_groups), axis=1)\n",
    "    return user_splits_df, user_splits_unique_df\n",
    "\n",
    "# Random prediction\n",
    "def predict_random_outfit(user_splits_df, user_splits_unique_df):\n",
    "    def get_random_outfits(x):\n",
    "        return np.random.choice(all_outfit_ids, NUM_ITEMS, replace=False)\n",
    "    all_outfit_ids = outfits_df[\"id\"].values\n",
    "    all_groups = outfits_df[\"group\"].values\n",
    "    user_splits_df[\"id_prediction\"] = user_splits_df.apply(lambda x: get_random_outfits(x), axis=1)\n",
    "    user_splits_df[\"group_prediction\"] = user_splits_df.apply(lambda x: np.random.choice(all_groups, NUM_ITEMS, replace=False), axis=1)\n",
    "    user_splits_unique_df[\"id_prediction\"] = user_splits_unique_df.apply(lambda x: get_random_outfits(x), axis=1)\n",
    "    user_splits_unique_df[\"group_prediction\"] = user_splits_unique_df.apply(lambda x: np.random.choice(all_groups, NUM_ITEMS, replace=False), axis=1)\n",
    "\n",
    "    return user_splits_df, user_splits_unique_df\n",
    "\n",
    "\n",
    "# Choose the method to use here\n",
    "METHOD = \"Rep\"\n",
    "if METHOD == \"Pop\":\n",
    "    user_splits_df, user_splits_unique_df = predict_most_popular(user_splits_df, user_splits_unique_df)\n",
    "elif METHOD == \"Rep\":\n",
    "    user_splits_df, user_splits_unique_df = predict_previous_rental(user_splits_df, user_splits_unique_df)\n",
    "elif METHOD == \"Rep + Pop\":\n",
    "    user_splits_df, user_splits_unique_df = predict_rental_and_most_popular(user_splits_df, user_splits_unique_df)\n",
    "elif METHOD == \"Rand\":\n",
    "    user_splits_df, user_splits_unique_df = predict_random_outfit(user_splits_df, user_splits_unique_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline evaluation for method: Rep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id_hit_rate_at_100       0.125512\n",
       "id_hit_rate_at_10        0.054559\n",
       "group_hit_rate_at_100    0.153176\n",
       "group_hit_rate_at_10     0.075564\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "id_hit_rate_at_100       0.0\n",
       "id_hit_rate_at_10        0.0\n",
       "group_hit_rate_at_100    0.0\n",
       "group_hit_rate_at_10     0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Evaluate the hit rate at n for a single user\n",
    "def evaluate_hit_rate_at_n(test_id, predicted_ids, n=10):\n",
    "    if predicted_ids is np.nan:\n",
    "        print(f\"None prediction for {test_id}!\")\n",
    "        return 0\n",
    "    predicted_ids = predicted_ids[:n]\n",
    "    if type(test_id) == str or type(test_id) == np.str_:\n",
    "        if test_id in predicted_ids:\n",
    "            return 1\n",
    "    elif type(test_id) == list or type(test_id) == np.ndarray:\n",
    "        for outfit_id in test_id:\n",
    "            if outfit_id in predicted_ids:\n",
    "                return 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown type {type(test_id)}\")\n",
    "    return 0\n",
    "\n",
    "# Evaluate the hit rate at n for all dataframes\n",
    "def evaluate_df_hit_rate_at_n(df, n=10):\n",
    "    HIT_RATE_COLUMNS = [\"id_hit_rate_at_100\", \"id_hit_rate_at_10\", \"group_hit_rate_at_100\", \"group_hit_rate_at_10\"]\n",
    "    df[\"id_hit_rate_at_100\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_outfit_id\"], x[\"id_prediction\"], n=100), axis=1)\n",
    "    df[\"id_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_outfit_id\"], x[\"id_prediction\"], n=10), axis=1)\n",
    "    df[\"group_hit_rate_at_100\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_group\"], x[\"group_prediction\"], n=100), axis=1)\n",
    "    df[\"group_hit_rate_at_10\"] = df.apply(lambda x: evaluate_hit_rate_at_n(x[\"test_group\"], x[\"group_prediction\"], n=10), axis=1)\n",
    "    display(df[HIT_RATE_COLUMNS].mean())\n",
    "    result_dict = {column: df[column].mean() for column in HIT_RATE_COLUMNS}\n",
    "    return df, result_dict\n",
    "\n",
    "print(f\"Baseline evaluation for method: {METHOD}\")\n",
    "user_splits_df, all_dict = evaluate_df_hit_rate_at_n(user_splits_df, n=10)\n",
    "user_splits_unique_df, ind_dict = evaluate_df_hit_rate_at_n(user_splits_unique_df, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rep Ind & 0.0546 & 0.1255 & 0.0000 & 0.0000 \\\\\n",
      "Rep Groups & 0.0756 & 0.1532 & 0.0000 & 0.0000 \\\\\\hline\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pyperclip\n",
    "\n",
    "# A small function to format the results into the format of the latex table in the article.\n",
    "def format_dicts_into_latex(all_dict, ind_dict, precision=4, run_name=\"Random\"):\n",
    "    first_row = f\"{run_name} Ind & {all_dict['id_hit_rate_at_10']:.{precision}f} & {all_dict['id_hit_rate_at_100']:.{precision}f} & {ind_dict['id_hit_rate_at_10']:.{precision}f} & {ind_dict['id_hit_rate_at_100']:.{precision}f} \\\\\\\\\"\n",
    "    second_row = f\"{run_name} Groups & {all_dict['group_hit_rate_at_10']:.{precision}f} & {all_dict['group_hit_rate_at_100']:.{precision}f} & {ind_dict['group_hit_rate_at_10']:.{precision}f} & {ind_dict['group_hit_rate_at_100']:.{precision}f} \\\\\\\\\\\\hline\"\n",
    "    full_string = first_row + \"\\n\" + second_row + \"\\n\"\n",
    "    print(full_string)\n",
    "    pyperclip.copy(full_string)\n",
    "\n",
    "format_dicts_into_latex(all_dict, ind_dict, precision=4, run_name=METHOD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
